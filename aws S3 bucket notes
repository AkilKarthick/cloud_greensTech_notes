__aws S3 bucket notes__
=====================

s3- simple storage servie -- it is an unlimited storage

Types of storage in aws
1.Block-
2.Object- s3
3.File-

so i can store anything as an object is S3, 
most cheapest,cost kammi, 
high availalbitlit  &
durability ()

in S3 we have two concept::
        1.bucket-like a file system are contaiiner for data sotred in s3
        2.object- file & folder that we store on file system

for each aws account we can able to create "100" bucket only
but we can create any number of object on a bucket so it is unlimited!!

1 acc --> 100 bucket
1 object --> 5tb (size)

if we reached the limit 100 bucket per account , if we need additional 20 bucket then
we can reachout to aws support & tell the requirement the it's possible

s3
scalable storage in the cloud

we have
        1.general purpose bucket - it's like a ware house
        2.directory bucket - structural bucket
                                eg college related data;s on colg direcotory , clg id . student id. etc
                                each session data's present here
======================================================

how to create a s3 bucket || General purpose bucket ||

step1: select region first eg uk or india whateve
step2: bucket name should be unique, we have option like copy setting from existing bucket- option is there\
step3: object ownership, ACL should disabled(recommended)
        we have ACL access control list
        if this ACL is enable then, object'sin the bucket can be owned by other AWS account.
        access to the bucket and its object can be specififed by ACL.
if it is disable: then i have the privligedd to allow the people whom all are need to use

step4: 
we have public bucket- 
        private bucket-

Bucket --   object       --res
public      public   -everyone can accesss bucket
public      private  -only bucket accees is there, but object is private so obj is not accessible,only perimssion user can access that
private     private  -only permission user are allowded to access the bucket
private     public   -you have to go inside bukcet first to access obj so not acessible right?
                      obj is not accessible,
                      but each objECT has an unique URL . we can access over there using that URL.

step5: block public access- to make your bucket private
        if you are planning to host a static webiste, unchekc this bo so that
        everyone can access your website.

step6:bucket versioning
      If Versioning is enabled → restore from previous versions.
        If not versioned and no backup → recovery not possible.

  
        
step7: we have encryption option
step8: click on create bucket

**-----------------------------**
uplod a file in Amazon s3 bucket

clic on bucket name that you have created, go to the Objec tab
clic on upload button to upload file or create folder to create subflder 
inside the bucket




after uploading an file
we need to select 
storage class on properties file

standard              - adikari use panura data \ frequenty accessed data morethan once a month wih millisecond acccess
intellignet tiering   - 
one zone IA
glacier instant retreievla
glacier flexible


finally try to delete that budet
permanently delet the object

now go to bucket> go to properties> enable bucket version > SAVE changes

now try to delet the file> you may get delete object type instead of permenantly delete
Delete marker  > you can retireve your object by deleting this / enable version
we have several option to do with our object
go to properties on object
1.encruption  = using key
2. Inteligent tiering archieve configuration = 
                we can create a poilicy like after 30 days it move to standerd..

3.server access logging = use cloud watch, to check bucket access log to check health of log
4.cloud trail data event = to check  acces log of object level

5. Event Notification = sen a nofiti when specific event occuer on you bucket
6. Transfer Accelerarion - for fast transfer
        what is the differenc between 
        ssd - input output file transfer speed 
        hardwar - some bit slow
7. object lock - write once read many (worm) only one time we can edit and many time read
8. Requestor Pay -  pay to see my bucket like anonymouse access to the bucket is disabledd

permission

1.buket policy =  for eg specifically you need to privide access s3 but this bucket access revoking
        to give speicific bucket acees 
2. metrices
        total bucket size
        total nmber of objects

Management:
1. lifecycle configuation = to set an object lifyclye
2. replication rule - to copy the same bucket on some other area like duplicate or replica

Access point::
for eg this is my bucket name : 29-07-2025akil
but i gave access point name like below,
29-07-2025akil-1  - i can give this to one user
29-07-2025akil-2  - to another user
29-07-2025akil-3- ..

so that gave that access point to different persons dev,tester,etc


================
interview questions,

Absolutely! For **3.5 years of experience in AWS with a focus on S3 (Simple Storage Service)**, you can expect **scenario-based questions** that go beyond basic bucket creation. Below are **practical, real-time S3 questions** typically asked in mid-level cloud/DevOps interviews:

---

### ✅ **Scenario-Based S3 Interview Questions for 3.5 YOE**

---

#### **1. You uploaded a file to an S3 bucket but are getting 403 Access Denied when trying to download it. What will you check?**

**Expected Answer:**

* Check if the object exists.
* Verify bucket and object **permissions (ACLs)**.
* Check the **bucket policy** for public/private access rules.
* Confirm the **IAM role/policy** of the user has `s3:GetObject`.
* If using **VPC endpoint**, check policy restrictions.

---

#### **2. How would you make an S3 bucket publicly readable for a website?**

**Expected Answer:**

* Add a bucket **policy** that allows `"Effect": "Allow", "Principal": "*", "Action": "s3:GetObject"` on `arn:aws:s3:::your-bucket/*`
* Enable **Static Website Hosting** in bucket properties.
* Ensure all objects are **public**, or use CloudFront for controlled access.

---

#### **3. Your application needs to trigger a Lambda function whenever a file is uploaded to S3. How would you implement this?**

**Expected Answer:**

* Configure **S3 event notification** for `s3:ObjectCreated:*`.
* Set the destination to a **Lambda function ARN**.
* Ensure Lambda has `s3:Read` permissions.
* Use filter rules if required (e.g., `.jpg` only).

---

#### **4. How would you securely share an S3 file with an external user for 24 hours only?**

**Expected Answer:**

* Generate a **pre-signed URL** with a 24-hour expiry.
* Use AWS CLI or SDK (`generate_presigned_url()`).
* Ensure the IAM role/user generating it has `s3:GetObject`.

---

#### **5. How do you implement version control in S3 and what are its benefits?**

**Expected Answer:**

* Enable **Versioning** on the bucket.
* Each object has unique version ID.
* Prevents **accidental overwrites/deletion**.
* Enables **rollbacks** to previous versions.

---

#### **6. A user accidentally deleted important files from S3. Can you recover them?**

**Expected Answer:**

* If **Versioning** is enabled → restore from previous versions.
* If **Object Lock (WORM)** was enabled → undeletable during retention period.
* If **not versioned** and no backup → recovery not possible.

---

#### **7. You want to automatically delete files older than 30 days from a bucket. What would you do?**

**Expected Answer:**

* Set up **Lifecycle Policy**:

  * Action: Expire object
  * Condition: Object age > 30 days
* Optionally, move to **Glacier** or **Intelligent-Tiering** first to save cost.

---

#### **8. What’s the difference between S3 Standard, S3 Infrequent Access, and Glacier? When do you use each?**

**Expected Answer:**

* **Standard**: For frequently accessed data
* **IA**: Accessed less frequently (monthly); cheaper storage, higher access cost
* **Glacier**: Archival, long-term storage; high latency (minutes to hours)
* Use Lifecycle rules to automate transitions.

---

#### **9. You want only a certain IP range (your office) to access your S3 bucket. How do you restrict it?**

**Expected Answer:**

* Apply a **bucket policy** with `"Condition": { "IpAddress": { "aws:SourceIp": "x.x.x.x/24" } }`
* Deny all other access using explicit `"Deny"` rules.

---

#### **10. A user is uploading large files to S3 and it fails midway. How can you make the upload more reliable?**

**Expected Answer:**

* Use **Multipart Upload**:

  * Splits large file into parts and uploads in parallel.
  * More reliable, especially for network errors.
  * Resume only failed parts.
* Use AWS SDK or CLI `s3api create-multipart-upload`.

---

Would you like these in **Excel format** to track your daily practice progress or self-testing?












